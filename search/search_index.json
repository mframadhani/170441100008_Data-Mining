{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Mining Pengertian Data Mining adalah proses yang menggunakan teknik statistik, matematika, kecerdasan buatan, machine learning untuk mengekstraksi dan mengidentifikasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar (Turban dkk. 2005). Terdapat beberapa istilah lain yang memiliki makna sama dengan data mining, yaitu Knowledge discovery in databases (KDD), ekstraksi pengetahuan ( knowledge extraction ), Analisa data/pola ( data/pattern analysis ), kecerdasan bisnis ( business intelligence ) dan data archaeology dan data dredging (Larose, 2005). Kemampuan Data mining untuk mencari informasi bisnis yang berharga dari basis data yang sangat besar, dapat dianalogikan dengan penambangan logam mulia dari lahan sumbernya, teknologi ini dipakai untuk: Prediksi trend dan sifat-sifat bisnis, dimana data mining mengotomatisasi proses pencarian informasi pemprediksi di dalam basis data yang besar. Penemuan pola-pola yang tidak diketahui sebelumnya, dimana data mining menyapu basis data, kemudian mengidentifikasi pola-pola yang sebelumnya tersembunyi dalam satu sapuan. Data mining berguna untuk membuat keputusan yang kritis, terutama dalam strategi. Fungsi Data Mining Data mining mempunyai fungsi yang penting untuk membantu mendapatkan informasi yang berguna serta meningkatkan pengetahuan bagi pengguna. Pada dasarnya, data mining mempunyai empat fungsi dasar yaitu: Fungsi Prediksi (prediction) . Proses untuk menemukan pola dari data dengan menggunakan beberapa variabel untuk memprediksikan variabel lain yang tidak diketahui jenis atau nilainya. Fungsi Deskripsi (description) . Proses untuk menemukan suatu karakteristik penting dari data dalam suatu basis data. Fungsi Klasifikasi (classification) . Klasifikasi merupakan suatu proses untuk menemukan model atau fungsi untuk menggambarkan class atau konsep dari suatu data. Proses yang digunakan untuk mendeskripsikan data yang penting serta dapat meramalkan kecenderungan data pada masa depan. Fungsi Asosiasi (association) . Proses ini digunakan untuk menemukan suatu hubungan yang terdapat pada nilai atribut dari sekumpulan data. Fungsi Klaster (clustering) . Clustering merupakan pengelompokan data tanpa berdasarkan kelas data tertentu ke dalam kelas objek yang sama. Sebuah kluster adalah kumpulan record yang memiliki kemiripan suatu dengan yang lainnya dan memiliki ketidakmiripan dengan record dalam kluster lain. Semakin besar kemiripan objek dalam suatu cluster dan semakin besar perbedaan tiap cluster maka kualitas analisis cluster semakin baik. Referensi Turban, E, 2005, Decision Support Systems and Intelligent Systems Edisi Bahasa Indonesia Jilid 1 . Andi: Yogyakarta. Larose, Daniel T. 2005. Discovering Knowledge in Data : An Introduction to Data Mining . John Willey & Sons, Inc. ayyad, Usama. 1996. Advances in Knowledge Discovery and Data Mining . MIT Press.","title":"Pengantar"},{"location":"#data-mining","text":"","title":"Data Mining"},{"location":"#pengertian","text":"Data Mining adalah proses yang menggunakan teknik statistik, matematika, kecerdasan buatan, machine learning untuk mengekstraksi dan mengidentifikasi informasi yang bermanfaat dan pengetahuan yang terkait dari berbagai database besar (Turban dkk. 2005). Terdapat beberapa istilah lain yang memiliki makna sama dengan data mining, yaitu Knowledge discovery in databases (KDD), ekstraksi pengetahuan ( knowledge extraction ), Analisa data/pola ( data/pattern analysis ), kecerdasan bisnis ( business intelligence ) dan data archaeology dan data dredging (Larose, 2005). Kemampuan Data mining untuk mencari informasi bisnis yang berharga dari basis data yang sangat besar, dapat dianalogikan dengan penambangan logam mulia dari lahan sumbernya, teknologi ini dipakai untuk: Prediksi trend dan sifat-sifat bisnis, dimana data mining mengotomatisasi proses pencarian informasi pemprediksi di dalam basis data yang besar. Penemuan pola-pola yang tidak diketahui sebelumnya, dimana data mining menyapu basis data, kemudian mengidentifikasi pola-pola yang sebelumnya tersembunyi dalam satu sapuan. Data mining berguna untuk membuat keputusan yang kritis, terutama dalam strategi.","title":"Pengertian"},{"location":"#fungsi-data-mining","text":"Data mining mempunyai fungsi yang penting untuk membantu mendapatkan informasi yang berguna serta meningkatkan pengetahuan bagi pengguna. Pada dasarnya, data mining mempunyai empat fungsi dasar yaitu: Fungsi Prediksi (prediction) . Proses untuk menemukan pola dari data dengan menggunakan beberapa variabel untuk memprediksikan variabel lain yang tidak diketahui jenis atau nilainya. Fungsi Deskripsi (description) . Proses untuk menemukan suatu karakteristik penting dari data dalam suatu basis data. Fungsi Klasifikasi (classification) . Klasifikasi merupakan suatu proses untuk menemukan model atau fungsi untuk menggambarkan class atau konsep dari suatu data. Proses yang digunakan untuk mendeskripsikan data yang penting serta dapat meramalkan kecenderungan data pada masa depan. Fungsi Asosiasi (association) . Proses ini digunakan untuk menemukan suatu hubungan yang terdapat pada nilai atribut dari sekumpulan data. Fungsi Klaster (clustering) . Clustering merupakan pengelompokan data tanpa berdasarkan kelas data tertentu ke dalam kelas objek yang sama. Sebuah kluster adalah kumpulan record yang memiliki kemiripan suatu dengan yang lainnya dan memiliki ketidakmiripan dengan record dalam kluster lain. Semakin besar kemiripan objek dalam suatu cluster dan semakin besar perbedaan tiap cluster maka kualitas analisis cluster semakin baik.","title":"Fungsi Data Mining"},{"location":"#referensi","text":"Turban, E, 2005, Decision Support Systems and Intelligent Systems Edisi Bahasa Indonesia Jilid 1 . Andi: Yogyakarta. Larose, Daniel T. 2005. Discovering Knowledge in Data : An Introduction to Data Mining . John Willey & Sons, Inc. ayyad, Usama. 1996. Advances in Knowledge Discovery and Data Mining . MIT Press.","title":"Referensi"},{"location":"bio/","text":"Biodata Penulis Nama : Moh Fajar Ramadhani N.I.M : 170441100008 Mata Kuliah : Data Mining (SI 6A) Program Studi : Sistem Informasi Jurusan : Teknik Informatika Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Biografi"},{"location":"bio/#biodata-penulis","text":"Nama : Moh Fajar Ramadhani N.I.M : 170441100008 Mata Kuliah : Data Mining (SI 6A) Program Studi : Sistem Informasi Jurusan : Teknik Informatika Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Biodata Penulis"},{"location":"indeksdecisiontree/","text":"Decision Tree Pendahuluan Pengertian Decision Tree Pohon keputusan adalah salah satu metode klasifikasi yang paling populer karena mudah untuk diinterpretasi oleh manusia. Pohon keputusan adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Sering terjadi tawar menawar antara keakuratan model dengan transparansi model. Dalam beberapa aplikasi, akurasi dari sebuah klasifikasi atau prediksi adalah satu-satunya hal yang ditonjolkan, misalnya sebuah perusahaan direct mail membuat sebuah model yang akurat untuk memprediksi anggota mana yang berpotensi untuk merespon permintaan, tanpa memperhatikan bagaimana atau mengapa model tersebut bekerja. Struktur Decision Tree Decision tree dibentuk dari 3 tipe dari simpul: simpul root, simpul perantara, dan simpul leaf. Simpul leaf memuat suatu keputusan akhir atau kelas target untuk suatu pohon keputusan. Simpul root adalah tiitk awal dari suatu decision tree. Setiap simpul perantara berhubungan dengan suatu pertanyaan atau pengujian. Algoritma Pohon dibangun dalam suatu metoda rekursif topdown divide and-conquer. Seluruh contoh pelatihan dimulai dari simpul root, lalu dilakukan penujian. Mencabang ke jalur yang benar berdasarkan hasil pengujian. Apakah simpul leaf ditemukan? Jika true , masukkan ke kelas target, jika false kembali ke langkah awal. Atribut-atribut berada dalam suatu kategori (jika bernilai kontinu, nilai-nilai tersebut didistribusikan terlebih dahulu). Contoh-contoh dipartisi secara rekursif berdasarkan atribut terpilih. Atribut-atribut uji dipilih berdasarakn heuristik atau pengukurann statistik (misal, information gain ). Rumus Menghitung *Entrophy * S : Himpunan kasus k : Jumlah partisi S Pj : Probabilitas yang didapat dari jumlah (Ya/Tidak) dibagi total kasus Menghitung *Gain * S : Himpunan kasus A : Atribut n : jumlah partisi atribut A |Si| : jumlah kasus pada partisi ke-i |S| : jumlah kasus dalam S Contoh Kasus Disini setiap percabangan menyatakan kondisi yang harus dipenuhi dan tiap ujung pohon menyatakan kelas data. Contoh di Gambar 1 adalah identifikasi pembeli komputer,dari pohon keputusan tersebut diketahui bahwa salah satu kelompok yang potensial membeli komputer adalah orang yang berusia di bawah 30 tahun dan juga pelajar. Setelah sebuah pohon keputusan dibangun maka dapat digunakan untuk mengklasifikasikan record yang belum ada kelasnya. Dimulai dari node root , menggunakan tes terhadap atribut dari record yang belum ada kelasnya tersebut lalu mengikuti cabang yang sesuai dengan hasil dari tes tersebut, yang akan membawa kepada internal node ( node yang memiliki satu cabang masuk dan dua atau lebih cabang yang keluar), dengan cara harus melakukan tes lagi terhadap atribut atau node daun. Record yang kelasnya tidak diketahui kemudian diberikan kelas yang sesuai dengan kelas yang ada pada node daun. Pada pohon keputusan setiap simpul daun menandai label kelas. Proses dalam pohon keputusan yaitu mengubah bentuk data (tabel) menjadi model pohon ( tree ) kemudian mengubah model pohon tersebut menjadi aturan ( rule ). Kelebihan Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode decision tree maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode decision tree ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional. Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode decision tree dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan. Kekurangan Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah decision tree yang besar. Kesulitan dalam mendesain decision tree yang optimal. Hasil kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain. Implementasi (Studi Kasus) Alat & Bahan Sebelum menerapkan konsep decision tree pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Balance Scales adalah mengklasifikasikan beberapa poin perolehan suatu percobaan dengan 3 klasifikasi objek. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan konsep Decision Tree Clasification. Langkah-langkah: Pertama Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. seaborn => library untuk membuat grafik statistik. pydotplus => library untuk memvisualisasikan bentuk hirarki. #import library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import accuracy_score import seaborn as sns from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus import numpy as np Kedua Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_csv('balance_scale.csv') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook Ketiga Menampilkan data. #explore data data.head() Keempat Melihat info kolom dari data. data.info() Kelima Memilih kolom uji untuk dihitung hasilnya. zero_not_accepted = ['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean) Keenam Membagi data train dan data test dengan data test 30%. X = data.iloc[:,0:3] #memilih objek data X dengan array y = data.iloc[:,3] #memilih objek data y dengan array #build model & train data X = data[['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan']] #objek uji y = data['seimbang'] #objek kelas #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0) Ketujuh Menentukan entropy data. clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4) #entropy dengan 4 cabang clf = clf.fit(X_train,y_train) y_pred = clf.predict(X_test) Kedelapan Meenentukan simpul root, simpul perantara, dan simpul leaf dari data yang telah diketahui nilai entropy-nya. feature_cols = ['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan'] #kolom yang diuji #mengelompokkan data ke dalam kelas dot_data = StringIO() export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['B','R','L']) #visualisai pohon keputusan graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) #menyimpan hasil visualisasi graph.write_png('keseimbangan.png') Image(graph.create_png()) Referensi Discovering Knowledge in Data (Introduction to Data Mining), Chapter 6, Daniel T. Larose, Wiley, 2004. Marwana. Algoritma C4.5 Untuk Simulasi Prediksi Kemenangan Dalam Pertandingan Sepakbola. Jurnal Informatika Multimedia, STIMED NUSA PALAPA. http://tessy.lecturer.pens.ac.id/kuliah/db2/klasifikasi.pdf . https://en.wikipedia.org/wiki/Decision_tree .","title":"Decision Tree"},{"location":"indeksdecisiontree/#decision-tree","text":"","title":"Decision Tree"},{"location":"indeksdecisiontree/#pendahuluan","text":"","title":"Pendahuluan"},{"location":"indeksdecisiontree/#pengertian-decision-tree","text":"Pohon keputusan adalah salah satu metode klasifikasi yang paling populer karena mudah untuk diinterpretasi oleh manusia. Pohon keputusan adalah model prediksi menggunakan struktur pohon atau struktur berhirarki. Konsep dari pohon keputusan adalah mengubah data menjadi pohon keputusan dan aturan-aturan keputusan. Manfaat utama dari penggunaan pohon keputusan adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simpel sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Pohon Keputusan juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Pohon keputusan memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Sering terjadi tawar menawar antara keakuratan model dengan transparansi model. Dalam beberapa aplikasi, akurasi dari sebuah klasifikasi atau prediksi adalah satu-satunya hal yang ditonjolkan, misalnya sebuah perusahaan direct mail membuat sebuah model yang akurat untuk memprediksi anggota mana yang berpotensi untuk merespon permintaan, tanpa memperhatikan bagaimana atau mengapa model tersebut bekerja. Struktur Decision Tree Decision tree dibentuk dari 3 tipe dari simpul: simpul root, simpul perantara, dan simpul leaf. Simpul leaf memuat suatu keputusan akhir atau kelas target untuk suatu pohon keputusan. Simpul root adalah tiitk awal dari suatu decision tree. Setiap simpul perantara berhubungan dengan suatu pertanyaan atau pengujian.","title":"Pengertian Decision Tree"},{"location":"indeksdecisiontree/#algoritma","text":"Pohon dibangun dalam suatu metoda rekursif topdown divide and-conquer. Seluruh contoh pelatihan dimulai dari simpul root, lalu dilakukan penujian. Mencabang ke jalur yang benar berdasarkan hasil pengujian. Apakah simpul leaf ditemukan? Jika true , masukkan ke kelas target, jika false kembali ke langkah awal. Atribut-atribut berada dalam suatu kategori (jika bernilai kontinu, nilai-nilai tersebut didistribusikan terlebih dahulu). Contoh-contoh dipartisi secara rekursif berdasarkan atribut terpilih. Atribut-atribut uji dipilih berdasarakn heuristik atau pengukurann statistik (misal, information gain ).","title":"Algoritma"},{"location":"indeksdecisiontree/#rumus","text":"Menghitung *Entrophy * S : Himpunan kasus k : Jumlah partisi S Pj : Probabilitas yang didapat dari jumlah (Ya/Tidak) dibagi total kasus Menghitung *Gain * S : Himpunan kasus A : Atribut n : jumlah partisi atribut A |Si| : jumlah kasus pada partisi ke-i |S| : jumlah kasus dalam S Contoh Kasus Disini setiap percabangan menyatakan kondisi yang harus dipenuhi dan tiap ujung pohon menyatakan kelas data. Contoh di Gambar 1 adalah identifikasi pembeli komputer,dari pohon keputusan tersebut diketahui bahwa salah satu kelompok yang potensial membeli komputer adalah orang yang berusia di bawah 30 tahun dan juga pelajar. Setelah sebuah pohon keputusan dibangun maka dapat digunakan untuk mengklasifikasikan record yang belum ada kelasnya. Dimulai dari node root , menggunakan tes terhadap atribut dari record yang belum ada kelasnya tersebut lalu mengikuti cabang yang sesuai dengan hasil dari tes tersebut, yang akan membawa kepada internal node ( node yang memiliki satu cabang masuk dan dua atau lebih cabang yang keluar), dengan cara harus melakukan tes lagi terhadap atribut atau node daun. Record yang kelasnya tidak diketahui kemudian diberikan kelas yang sesuai dengan kelas yang ada pada node daun. Pada pohon keputusan setiap simpul daun menandai label kelas. Proses dalam pohon keputusan yaitu mengubah bentuk data (tabel) menjadi model pohon ( tree ) kemudian mengubah model pohon tersebut menjadi aturan ( rule ).","title":"Rumus"},{"location":"indeksdecisiontree/#kelebihan","text":"Daerah pengambilan keputusan yang sebelumnya kompleks dan sangat global, dapat diubah menjadi lebih simpel dan spesifik. Eliminasi perhitungan-perhitungan yang tidak diperlukan, karena ketika menggunakan metode decision tree maka sample diuji hanya berdasarkan kriteria atau kelas tertentu. Fleksibel untuk memilih fitur dari internal node yang berbeda, fitur yang terpilih akan membedakan suatu kriteria dibandingkan kriteria yang lain dalam node yang sama. Kefleksibelan metode decision tree ini meningkatkan kualitas keputusan yang dihasilkan jika dibandingkan ketika menggunakan metode penghitungan satu tahap yang lebih konvensional. Dalam analisis multivariat, dengan kriteria dan kelas yang jumlahnya sangat banyak, seorang penguji biasanya perlu untuk mengestimasikan baik itu distribusi dimensi tinggi ataupun parameter tertentu dari distribusi kelas tersebut. Metode decision tree dapat menghindari munculnya permasalahan ini dengan menggunakan criteria yang jumlahnya lebih sedikit pada setiap node internal tanpa banyak mengurangi kualitas keputusan yang dihasilkan.","title":"Kelebihan"},{"location":"indeksdecisiontree/#kekurangan","text":"Terjadi overlap terutama ketika kelas-kelas dan criteria yang digunakan jumlahnya sangat banyak. Hal tersebut juga dapat menyebabkan meningkatnya waktu pengambilan keputusan dan jumlah memori yang diperlukan. Pengakumulasian jumlah eror dari setiap tingkat dalam sebuah decision tree yang besar. Kesulitan dalam mendesain decision tree yang optimal. Hasil kualitas keputusan yang didapatkan dari metode decision tree sangat tergantung pada bagaimana pohon tersebut didesain.","title":"Kekurangan"},{"location":"indeksdecisiontree/#implementasi-studi-kasus","text":"","title":"Implementasi (Studi Kasus)"},{"location":"indeksdecisiontree/#alat-bahan","text":"Sebelum menerapkan konsep decision tree pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Balance Scales adalah mengklasifikasikan beberapa poin perolehan suatu percobaan dengan 3 klasifikasi objek. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan konsep Decision Tree Clasification.","title":"Alat &amp; Bahan"},{"location":"indeksdecisiontree/#langkah-langkah","text":"","title":"Langkah-langkah:"},{"location":"indeksdecisiontree/#pertama","text":"Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. seaborn => library untuk membuat grafik statistik. pydotplus => library untuk memvisualisasikan bentuk hirarki. #import library import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics from sklearn.metrics import accuracy_score import seaborn as sns from sklearn.tree import export_graphviz from sklearn.externals.six import StringIO from IPython.display import Image from sklearn.tree import export_graphviz import pydotplus import numpy as np","title":"Pertama"},{"location":"indeksdecisiontree/#kedua","text":"Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_csv('balance_scale.csv') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook","title":"Kedua"},{"location":"indeksdecisiontree/#ketiga","text":"Menampilkan data. #explore data data.head()","title":"Ketiga"},{"location":"indeksdecisiontree/#keempat","text":"Melihat info kolom dari data. data.info()","title":"Keempat"},{"location":"indeksdecisiontree/#kelima","text":"Memilih kolom uji untuk dihitung hasilnya. zero_not_accepted = ['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan'] # for col in zero_not_accepted: # for i in data[col]: # if i==0: # colSum = sum(data[col]) # meanCol=colSum/len(data[col]) # data[col]=meanCol for col in zero_not_accepted: data[col]= data[col].replace(0,np.NaN) mean = int(data[col].mean(skipna=True)) data[col] = data[col].replace(np.NaN,mean)","title":"Kelima"},{"location":"indeksdecisiontree/#keenam","text":"Membagi data train dan data test dengan data test 30%. X = data.iloc[:,0:3] #memilih objek data X dengan array y = data.iloc[:,3] #memilih objek data y dengan array #build model & train data X = data[['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan']] #objek uji y = data['seimbang'] #objek kelas #split data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)","title":"Keenam"},{"location":"indeksdecisiontree/#ketujuh","text":"Menentukan entropy data. clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4) #entropy dengan 4 cabang clf = clf.fit(X_train,y_train) y_pred = clf.predict(X_test)","title":"Ketujuh"},{"location":"indeksdecisiontree/#kedelapan","text":"Meenentukan simpul root, simpul perantara, dan simpul leaf dari data yang telah diketahui nilai entropy-nya. feature_cols = ['berat_kiri','jarak_kiri','berat_kanan','jarak_kanan'] #kolom yang diuji #mengelompokkan data ke dalam kelas dot_data = StringIO() export_graphviz(clf, out_file=dot_data, filled=True, rounded=True, special_characters=True,feature_names = feature_cols,class_names=['B','R','L']) #visualisai pohon keputusan graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) #menyimpan hasil visualisasi graph.write_png('keseimbangan.png') Image(graph.create_png())","title":"Kedelapan"},{"location":"indeksdecisiontree/#referensi","text":"Discovering Knowledge in Data (Introduction to Data Mining), Chapter 6, Daniel T. Larose, Wiley, 2004. Marwana. Algoritma C4.5 Untuk Simulasi Prediksi Kemenangan Dalam Pertandingan Sepakbola. Jurnal Informatika Multimedia, STIMED NUSA PALAPA. http://tessy.lecturer.pens.ac.id/kuliah/db2/klasifikasi.pdf . https://en.wikipedia.org/wiki/Decision_tree .","title":"Referensi"},{"location":"indekskmeans/","text":"K-Means Pendahuluan Pengertian Data Clustering merupakan salah satu metode Data Mining yang bersifat tanpa arahan (unsupervised). Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/ kelompok. Metode ini mempartisi data ke dalam cluster/ kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. Manfaat Clustering adalah sebagai Identifikasi Object (Recognition) misalnya dalam bidang Image Processing, Computer Vision atau robot vision. Selain itu adalah sebagai Sistem Pendukung Keputusan dan Data Mining seperti Segmentasi pasar, pemetaan wilayah, Manajemen marketing dll. K-means clustering merupakan salah satu metode data clustering non-hirarki yang mengelompokan data dalam bentuk satu atau lebih cluster/kelompok. Data-data yang memiliki karakteristik yang sama dikelompokan dalam satu cluster/kelompok dan data yang memiliki karakteristik yang berbeda dikelompokan dengan cluster/kelompok yang lain sehingga data yang berada dalam satu cluster/kelompok memiliki tingkat variasi yang kecil (Agusta, 2007). Karakteristik K-Mean K-means sangat cepat dalam proses clustering. K-means sangat sensitive pada pembangkitan centroid awal secara random. Memungkinkan suatu cluster tidak mempunyai anggota. Hasil clustering dengan K-means bersifat unik (selalu berubah-ubah, terkadang baik, terkadang jelek). Tujuan Analisis Cluster Untuk mengelompokkan objek-objek (individu-individu) menjadi kelompok-kelompok yang mempunyai sifat yang relatif sama (homogen). Untuk membedakan dengan jelas antara satu kelompok ( cluster ) dengan kelompok lainnya. Manfaat Analisis Cluster Untuk menerapkan dasar-dasar pengelompokan dengan lebih konsisten. Untuk mengembangkan suatu metode generalisasi secara induktif, yaitu pengambilan kesimpulan secara umum dengan berdasarkan fakta-fakta khusus. Menemukan tipologi yang cocok dengan karakter obyek yang diteliti. Mendiskripsikan sifat-sifat/karakteristik dari masing-masing kelompok. Algoritma Data clustering menggunakan metode K-Means ini secara umum dilakukan dengan algoritma dasar sebagai berikut (Yudi Agusta, 2007) : Tentukan jumlah cluster. Alokasikan data ke dalam cluster secara random. Hitung centroid/ rata-rata dari data yang ada di masing-masing cluster. Alokasikan masing-masing data ke centroid/ rata-rata terdekat. Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan. Eucledian Distance Untuk mengelompokkan sebuah data pada kelompok tertentu, hal yang harus dilakukan adalah menghitung jarak antara data dengan centroid. Beberapa distance space telah diimplementasikan dalam menghitung jarak ( distance antara data dan centroid ) termasuk di antaranya L1 ( Manhattan/ City Block distance space , L2 ( Euclidean ) distance space , dan Lp ( Minkowski ) distance space . Jarak antara dua titik x1 dan x2 pada Manhattan/City Block distance space dihitung dengan menggunakan rumus sebagai berikut (Yudi Agusta, 2007): (1) (2) Dimana: D L2 : jarak kuadrat Euclidean antar onjek ke x 2 dengan x 1 P : jumlah variabel cluster x 2j : nilai atau data dari objek ke-2 pada variabel ke-j x 1j : nilai atau data dari obbjek ke-1 ada variabel ke-j (Everitt, 1993). Kelebihan Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan pembelajaran ini relatif cepat. Mudah untuk diadaptasi. Umum digunakan. Kekurangan Sebelum algoritma dijalankan, k buah titik diinisialisasi secara random sehingga pengelompokkan data yang dihasilkan dapat berbeda-beda. Jika nilai random untuk inisialisasi kurang baik, maka pengelompokkan yang dihasilkan pun menjadi kurang optimal. Dapat terjebak dalam masalah yang disebut curse of dimensionality . Hal ini dapat terjadi jika data pelatihan memiliki dimensi yang sangat tinggi (Contoh jika data pelatihan terdiri dari 2 atribut maka dimensinya adalah 2 dimensi. Namun jika ada 20 atribut, maka akan ada 20 dimensi). Salah satu cara kerja algoritma ini adalah mencari jarak terdekat antara k buah titik dengan titik lainnya. Jika mencari jarak antar titik pada 2 dimensi, masih mudah dilakukan. Namun bagaimana mencari jarak antar titik jika terdapat 20 dimensi. Hal ini akan menjadi sulit. Jika hanya terdapat beberapa titik sampel data, maka cukup mudah untuk menghitung dan mencari titik terdekat dengan k titik yang diinisialisasi secara random . Namun jika terdapat banyak sekali titik data (misalnya satu milyar buah data), maka perhitungan dan pencarian titik terdekat akan membutuhkan waktu yang lama. Proses tersebut dapat dipercepat, namun dibutuhkan struktur data yang lebih rumit seperti kD-Tree atau hashing . Implementasi (Studi Kasus) Alat & Bahan Sebelum menerapkan konsep k-mean clustering pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Air Qualiy adalah mengklasterkan beberapa senyawa yang ada pada udara tersebut dengan beberapa ketentuan. Untuk contoh penulisan program dan datanya bisa kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan K-Means Clustering. Langkah-langkah Pertama Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. matplotlib => untuk menyajikan visualisasi data cluster . #import library import pandas as pd import numpy as np import matplotlib.pylot as plt from sklearn.cluster import KMeans from sklearn.preprocessing import MinMaxScaler Kedua Mengimport data csv dari komputer dengan perintah pandas. #import data csv air = pd.read_csv('AirQualityItaly.csv') #pastikan file csv berada dalam satu folder dengan file python air.head() Ketiga Hapus label yang tidak dipakai untuk dianalisis. #menghapus label yang tidak terpakai air = air.drop(['Date','Time','CO(GT)','PT08.S1(CO)','C6H6(GT)','PT08.S2(NMHC)','NOx(GT)','PT08.S3(NOx)','NO2(GT)','PT08.S4(NO2)','PT08.S5(O3)','T','AH'], axis = 1]) Keempat Cek info tabel apakah label yang dihapus berhasil atau tidak. air.info() Kelima Mengelompokkan data dengan visualisasi. plt.scatter(air.NMHC, air.RH, s = 10, c = 'c', marker = '.', alpha = 1) plt.show() (Visualisasi data cluster ) Keenam Menentukan centroid . air_x = air.iloc[:, 0:2] air_x.head() x_array = np.array(air_x) print(x_array) scaler = MinMaxScaler() x_scaled = scaler.fit_transform(x_array) x_scaled Ketujuh Menentukan cluster = 5 kmeans = KMeans(n_clusters = 5, init='k-means++', max_iter=100, n_init=10, random_state=123) c_kmeans = kmeans.fit_predict(x_scaled) air[\"Kelas\"] = c_kmeans Kedelapan Menampilan hasil cluster print(air) print(kmeans.cluster_centers_) air['kluster'] = kmeans.labels_ Kesembilan Visualisasi data hasil akhir cluster output = plt.scatter(x_scaled[:,0], x_scaled[:,1], s = 100, c = air.kluster, marker = '.', alpha = 1, ) centers = kmeans.cluster_centers_ plt.scatter(centers[:,0], centers[:,1], c='red', label='Centroids', s=150, alpha=1 , marker='o'); plt.title('Hasil Klustering K-Means Kualitas Udara') plt.xlabel('Tingkat Kadar NHCM') plt.ylabel('Tingkat Kadar RH') plt.colorbar (output) plt.legend() plt.show() Visualisasi data 5 cluster dengan centroid -nya Refrensi Agusta, Y. 2007. K-means - Penerapan, Permasalahan dan Metode Terkait. Jurnal Sistem dan Informatika Vol. 3 (Februari 2007): 47-60. Santosa, B. 2007. Data Mining: Teknik Pemanfaatan Data untuk Keperluan Bisnis. Yogyakarta: Graha Ilmu. Dalam jurnal SNTIKI, vol (5), Hal 395-398, oleh Nengsih W, Febiyanto pada tahun 2012 dengan judul \u201cData Mining Analysis Pengelompokan Penerima Beasiswa Menggunakan Teknik Clustering K-Means. https://id.wikipedia.org/wiki/K-means https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97 https://www.academia.edu/36232466/IMPLEMENTASI_ALGORITMA_K-MEANS_CLUSTERING_DALAM_PENENTUAN_PRIORITAS_REHABILITASI_DAERAH_ALIRAN_SUNGAI_DAS","title":"K-Means"},{"location":"indekskmeans/#k-means","text":"","title":"K-Means"},{"location":"indekskmeans/#pendahuluan","text":"","title":"Pendahuluan"},{"location":"indekskmeans/#pengertian","text":"Data Clustering merupakan salah satu metode Data Mining yang bersifat tanpa arahan (unsupervised). Ada dua jenis data clustering yang sering dipergunakan dalam proses pengelompokan data yaitu hierarchical (hirarki) data clustering dan non-hierarchical (non hirarki) data clustering. K-Means merupakan salah satu metode data clustering non hirarki yang berusaha mempartisi data yang ada ke dalam bentuk satu atau lebih cluster/ kelompok. Metode ini mempartisi data ke dalam cluster/ kelompok sehingga data yang memiliki karakteristik yang sama dikelompokkan ke dalam satu cluster yang sama dan data yang mempunyai karakteristik yang berbeda dikelompokkan ke dalam kelompok yang lain. Adapun tujuan dari data clustering ini adalah untuk meminimalisasikan objective function yang diset dalam proses clustering, yang pada umumnya berusaha meminimalisasikan variasi di dalam suatu cluster dan memaksimalisasikan variasi antar cluster. Manfaat Clustering adalah sebagai Identifikasi Object (Recognition) misalnya dalam bidang Image Processing, Computer Vision atau robot vision. Selain itu adalah sebagai Sistem Pendukung Keputusan dan Data Mining seperti Segmentasi pasar, pemetaan wilayah, Manajemen marketing dll. K-means clustering merupakan salah satu metode data clustering non-hirarki yang mengelompokan data dalam bentuk satu atau lebih cluster/kelompok. Data-data yang memiliki karakteristik yang sama dikelompokan dalam satu cluster/kelompok dan data yang memiliki karakteristik yang berbeda dikelompokan dengan cluster/kelompok yang lain sehingga data yang berada dalam satu cluster/kelompok memiliki tingkat variasi yang kecil (Agusta, 2007).","title":"Pengertian"},{"location":"indekskmeans/#karakteristik-k-mean","text":"K-means sangat cepat dalam proses clustering. K-means sangat sensitive pada pembangkitan centroid awal secara random. Memungkinkan suatu cluster tidak mempunyai anggota. Hasil clustering dengan K-means bersifat unik (selalu berubah-ubah, terkadang baik, terkadang jelek).","title":"Karakteristik K-Mean"},{"location":"indekskmeans/#tujuan-analisis-cluster","text":"Untuk mengelompokkan objek-objek (individu-individu) menjadi kelompok-kelompok yang mempunyai sifat yang relatif sama (homogen). Untuk membedakan dengan jelas antara satu kelompok ( cluster ) dengan kelompok lainnya.","title":"Tujuan Analisis Cluster"},{"location":"indekskmeans/#manfaat-analisis-cluster","text":"Untuk menerapkan dasar-dasar pengelompokan dengan lebih konsisten. Untuk mengembangkan suatu metode generalisasi secara induktif, yaitu pengambilan kesimpulan secara umum dengan berdasarkan fakta-fakta khusus. Menemukan tipologi yang cocok dengan karakter obyek yang diteliti. Mendiskripsikan sifat-sifat/karakteristik dari masing-masing kelompok.","title":"Manfaat Analisis Cluster"},{"location":"indekskmeans/#algoritma","text":"Data clustering menggunakan metode K-Means ini secara umum dilakukan dengan algoritma dasar sebagai berikut (Yudi Agusta, 2007) : Tentukan jumlah cluster. Alokasikan data ke dalam cluster secara random. Hitung centroid/ rata-rata dari data yang ada di masing-masing cluster. Alokasikan masing-masing data ke centroid/ rata-rata terdekat. Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan.","title":"Algoritma"},{"location":"indekskmeans/#eucledian-distance","text":"Untuk mengelompokkan sebuah data pada kelompok tertentu, hal yang harus dilakukan adalah menghitung jarak antara data dengan centroid. Beberapa distance space telah diimplementasikan dalam menghitung jarak ( distance antara data dan centroid ) termasuk di antaranya L1 ( Manhattan/ City Block distance space , L2 ( Euclidean ) distance space , dan Lp ( Minkowski ) distance space . Jarak antara dua titik x1 dan x2 pada Manhattan/City Block distance space dihitung dengan menggunakan rumus sebagai berikut (Yudi Agusta, 2007): (1) (2) Dimana: D L2 : jarak kuadrat Euclidean antar onjek ke x 2 dengan x 1 P : jumlah variabel cluster x 2j : nilai atau data dari objek ke-2 pada variabel ke-j x 1j : nilai atau data dari obbjek ke-1 ada variabel ke-j (Everitt, 1993).","title":"Eucledian Distance"},{"location":"indekskmeans/#kelebihan","text":"Mudah untuk diimplementasikan dan dijalankan. Waktu yang dibutuhkan untuk menjalankan pembelajaran ini relatif cepat. Mudah untuk diadaptasi. Umum digunakan.","title":"Kelebihan"},{"location":"indekskmeans/#kekurangan","text":"Sebelum algoritma dijalankan, k buah titik diinisialisasi secara random sehingga pengelompokkan data yang dihasilkan dapat berbeda-beda. Jika nilai random untuk inisialisasi kurang baik, maka pengelompokkan yang dihasilkan pun menjadi kurang optimal. Dapat terjebak dalam masalah yang disebut curse of dimensionality . Hal ini dapat terjadi jika data pelatihan memiliki dimensi yang sangat tinggi (Contoh jika data pelatihan terdiri dari 2 atribut maka dimensinya adalah 2 dimensi. Namun jika ada 20 atribut, maka akan ada 20 dimensi). Salah satu cara kerja algoritma ini adalah mencari jarak terdekat antara k buah titik dengan titik lainnya. Jika mencari jarak antar titik pada 2 dimensi, masih mudah dilakukan. Namun bagaimana mencari jarak antar titik jika terdapat 20 dimensi. Hal ini akan menjadi sulit. Jika hanya terdapat beberapa titik sampel data, maka cukup mudah untuk menghitung dan mencari titik terdekat dengan k titik yang diinisialisasi secara random . Namun jika terdapat banyak sekali titik data (misalnya satu milyar buah data), maka perhitungan dan pencarian titik terdekat akan membutuhkan waktu yang lama. Proses tersebut dapat dipercepat, namun dibutuhkan struktur data yang lebih rumit seperti kD-Tree atau hashing .","title":"Kekurangan"},{"location":"indekskmeans/#implementasi-studi-kasus","text":"","title":"Implementasi (Studi Kasus)"},{"location":"indekskmeans/#alat-bahan","text":"Sebelum menerapkan konsep k-mean clustering pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Air Qualiy adalah mengklasterkan beberapa senyawa yang ada pada udara tersebut dengan beberapa ketentuan. Untuk contoh penulisan program dan datanya bisa kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan K-Means Clustering.","title":"Alat &amp; Bahan"},{"location":"indekskmeans/#langkah-langkah","text":"","title":"Langkah-langkah"},{"location":"indekskmeans/#pertama","text":"Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. matplotlib => untuk menyajikan visualisasi data cluster . #import library import pandas as pd import numpy as np import matplotlib.pylot as plt from sklearn.cluster import KMeans from sklearn.preprocessing import MinMaxScaler","title":"Pertama"},{"location":"indekskmeans/#kedua","text":"Mengimport data csv dari komputer dengan perintah pandas. #import data csv air = pd.read_csv('AirQualityItaly.csv') #pastikan file csv berada dalam satu folder dengan file python air.head()","title":"Kedua"},{"location":"indekskmeans/#ketiga","text":"Hapus label yang tidak dipakai untuk dianalisis. #menghapus label yang tidak terpakai air = air.drop(['Date','Time','CO(GT)','PT08.S1(CO)','C6H6(GT)','PT08.S2(NMHC)','NOx(GT)','PT08.S3(NOx)','NO2(GT)','PT08.S4(NO2)','PT08.S5(O3)','T','AH'], axis = 1])","title":"Ketiga"},{"location":"indekskmeans/#keempat","text":"Cek info tabel apakah label yang dihapus berhasil atau tidak. air.info()","title":"Keempat"},{"location":"indekskmeans/#kelima","text":"Mengelompokkan data dengan visualisasi. plt.scatter(air.NMHC, air.RH, s = 10, c = 'c', marker = '.', alpha = 1) plt.show() (Visualisasi data cluster )","title":"Kelima"},{"location":"indekskmeans/#keenam","text":"Menentukan centroid . air_x = air.iloc[:, 0:2] air_x.head() x_array = np.array(air_x) print(x_array) scaler = MinMaxScaler() x_scaled = scaler.fit_transform(x_array) x_scaled","title":"Keenam"},{"location":"indekskmeans/#ketujuh","text":"Menentukan cluster = 5 kmeans = KMeans(n_clusters = 5, init='k-means++', max_iter=100, n_init=10, random_state=123) c_kmeans = kmeans.fit_predict(x_scaled) air[\"Kelas\"] = c_kmeans","title":"Ketujuh"},{"location":"indekskmeans/#kedelapan","text":"Menampilan hasil cluster print(air) print(kmeans.cluster_centers_) air['kluster'] = kmeans.labels_","title":"Kedelapan"},{"location":"indekskmeans/#kesembilan","text":"Visualisasi data hasil akhir cluster output = plt.scatter(x_scaled[:,0], x_scaled[:,1], s = 100, c = air.kluster, marker = '.', alpha = 1, ) centers = kmeans.cluster_centers_ plt.scatter(centers[:,0], centers[:,1], c='red', label='Centroids', s=150, alpha=1 , marker='o'); plt.title('Hasil Klustering K-Means Kualitas Udara') plt.xlabel('Tingkat Kadar NHCM') plt.ylabel('Tingkat Kadar RH') plt.colorbar (output) plt.legend() plt.show() Visualisasi data 5 cluster dengan centroid -nya","title":"Kesembilan"},{"location":"indekskmeans/#refrensi","text":"Agusta, Y. 2007. K-means - Penerapan, Permasalahan dan Metode Terkait. Jurnal Sistem dan Informatika Vol. 3 (Februari 2007): 47-60. Santosa, B. 2007. Data Mining: Teknik Pemanfaatan Data untuk Keperluan Bisnis. Yogyakarta: Graha Ilmu. Dalam jurnal SNTIKI, vol (5), Hal 395-398, oleh Nengsih W, Febiyanto pada tahun 2012 dengan judul \u201cData Mining Analysis Pengelompokan Penerima Beasiswa Menggunakan Teknik Clustering K-Means. https://id.wikipedia.org/wiki/K-means https://www.codepolitan.com/5-library-python-untuk-data-science-59b774b6cad97 https://www.academia.edu/36232466/IMPLEMENTASI_ALGORITMA_K-MEANS_CLUSTERING_DALAM_PENENTUAN_PRIORITAS_REHABILITASI_DAERAH_ALIRAN_SUNGAI_DAS","title":"Refrensi"},{"location":"indeksknn/","text":"K-Nearest Neighbors Pendahuluan Pengertian K-Nearest Neighbor K-Nearest Neighbor (K-NN) adalah suatu metode yang menggunakan algoritma supervised dimana hasil dari sampel uji yang baru diklasifikasikan berdasarkan mayoritas dari kategori pada K-NN. Tujuan dari algoritma ini adalah mengklasifikasi objek baru berdasakan atribut dan sampel latih. pengklasifikasian tidak menggunakan model apapun untuk dicocokkan dan hanya berdasarkan pada memori. Diberikan titik uji, akan ditemukan sejumlah K objek (titik training) yang paling dekat dengan titik uji. Klasifikasi menggunakan voting terbanyak di antara klasifikasi dari K objek. Algoritma K-NN menggunakan klasifikasi ketetanggaan sebagai nilai prediksi dari sample uji yang baru. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Eucledian. Nilai k yang terbaik untuk algoritma ini tergantung pada data. Secara umum, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritma k-nearest neighbor. Ketepatan algoritma k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritma ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur agar performa klasifikasi menjadi lebih baik. Sesuai dengan prinsip kerja K-Nearest Neighbor yaitu mencari jarak terdekat antara data yang akan dievaluasi dengan k tetangga(neighbor) terdekatnya dalam data pelatihan. Persamaan dibawah ini menunjukkan rumus perhitungan untuk mencari jarak terdekat dengan d adalah jarak dan p adalah dimensi data(Agusta, 2007): Dengan keterangan : \ud835\udc651 : sampel data \ud835\udc652 : data uji i : data ke-i d : jarak euclidean p : dimensi data Algoritma Algoritma metode KNN sangatlah sederhana, bekerja berdasarkan jarak terpendek dari query instance ke training sample untuk menentukan KNN-nya. Training sample diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi training sample. Sebuah titik pada ruang ini ditandai kelac c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat dari titik tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan Euclidean Distance. Tentukan parameter K (jumlah tetangga paling dekat). Hitung kuadrat jarak euclid masing-masing objek terhadap data sample yang diberikan. Urutkan objek-objek kedalam kelompok yang memiliki jarak terkecil. Kumpulkan kategori Y (klasifikasi nearest neighbor). Dengan kategori nearest neighbor yang paling banyak, maka dapt diprediksikan nilai query instance yang telah dihitung. Contoh Kasus (Hitung Manual) Data didapatkan dari kuesioner dengan obyek pengujian berupa dua atribut (daya tahan keasaman dan kekuatan) untuk mengklasifikasikan apakah sebuah kertas tissue tergolong bagus atau jelek. Berikut ini contoh datanya: X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m 2 ) Klasifikasi 7 7 Jelek 7 4 Jelek 3 4 Bagus 1 4 Bagus Sebuah pabrik memproduksi kertas tissue baru yang memiliki X1 = 3 dan X2 = 7. Kita gunakan algoritma KNN untuk melakukan prediksi termasuk klasifikasi apa (bagus atau jelek) kertas tissue yang baru ini. Jawaban Tentukan parameter K = jumlah banyaknya tetangga terdekat. Misal K=3. Hitung jarak antara data baru dan semua data yang ada di data training. Misal digunakan square distance dari jarak antara data baru dengan semua data yang ada di data training. X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) 7 7 (7-3) 2 +(7-7) 2 = 16 7 4 (7-3) 2 +(4-7) 2 = 25 3 4 (3-3) 2 +(4-7) 2 = 9 1 4 (1-3) 2 +(4-7) 2 = 13 Urutkan jarak tersebut dan tentukan tetangga mana yang terdekat berdasarkan jarak minimum ke-K. X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) Urutan Jarak K-3-NN? 7 7 (7-3)2+(7-7)2 = 16 3 Ya 7 4 (7-3)2+(4-7)2 = 25 4 Tidak 3 4 (3-3)2+(4-7)2 = 9 1 Ya 1 4 (1-3)2+(4-7)2 = 13 2 Ya Tentukan kategori dari tetangga terdekat. Perhatikan pada baris kedua pada kolom terakhir: katagori dari tetangga terdekat (Y) tidak termasuk karena ranking dari data ini lebih dari 3 (=K). X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) Urutan Jarak K-3-NN? Kategory K-NN 7 7 (7-3)2+(7-7)2 = 16 3 Ya Jelek 7 4 (7-3)2+(4-7)2 = 25 4 Tidak - 3 4 (3-3)2+(4-7)2 = 9 1 Ya Bagus 1 4 (1-3)2+(4-7)2 = 13 2 Ya Bagus Gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi dari data yang baru. Kita punya 2 kategori Bagus dan 1 kategori Jelek, karena 2>1 maka kita simpulkan bahwa kertas tissue baru tadi yang memiliki X1 = 3 dan X2 = 7 termasuk dalam kategori Bagus. Itulah contoh penerapan k-nn pada sebuah kasus kusioner terhadap kertas tissue. Kelebihan Mudah dipahami dan diimplementasikan. Sangan non-linear . K-NN merupakan salah satu algoritma (model) pembelajaran mesin yang bersifat non-parametrik , yaitu model yang tidak mengasumsikan apa-apa mengenai distribusi instance di dalam dataset . Model non-parametrik biasanya lebih sulit diinterpretasikan, namun salah satu kelebihannya adalah garis keputusan kelas yang dihasilkan model tersebut bisa jadi sangat fleksibel dan non-linear . Kuat dalam hal ruang pencarian, misalnya kelas tidak harus linear dipisahkan. Efektif apabila data training sample -nya besar. Tangguh terhadap data training sample -nya besar. Beberapa parameter untuk acuan: jarak metrik dan k. Memiliki konsistensi yang kuat. Kekurangan Perlu untuk menentukan nilai k yang optimal sehingga untuk menyatakan jumlah tetangga terdekatnya lebih mudah. Nilai komputasi yang cukup tinggi karena perhitungan jarak harus dilkukan pada setiap query instance. Tidak menangani missing value secara implisit. Sensitif terhadap data pencilan ( outlier ). Rentan terhadap variabel yang non-informatif. Gambar : Variabel yang noninformatif mengacaukan klasifikasi dalam k-nn Rentan terhadap dimensionalitas yang tinggi. Rentang terhadap perbedaan rentang variabel. Gambar : Perbedaan rentang variabel bisa mengacaukan klasifikasi k-nn Implementasi (Studi Kasus) Alat & Bahan Sebelum menerapkan konsep k-nearest neighbor pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Fruit Data adalah mengklasifikasikan beberapa jenis buah dengan 3 ketentuan inputan. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan K-NN Classification. Langkah-langkah Pertama Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. matplotlib => untuk menyajikan visualisasi data cluster . #import library import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn import neighbors from matplotlib.colors import ListedColormap, BoundaryNorm import matplotlib.patches as mpatches Kedua Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_table('fruit_data.txt') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook Ketiga Menampilkan data. #explore data print(data.shape) data.head(10) #Menampilkan 10 baris pertama dari tabel # membuat nilai kunci utama antara fruit_label dengan fruit_name lookup_fruit_name = dict(zip(data.fruit_label.unique(), data.fruit_name.unique())) print(lookup_fruit_name) Keempat Membuat model dan data train. X = data[['mass', 'width', 'height']] y = data['fruit_label'] Kelima Melakukan split (memisah) data antara data test dan data train. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0) Keenam Mengecek nilai dengan dimensi array. print('X_train = ', X_train.shape) print('X_test = ', X_test.shape) print('y_train = ', y_train.shape) print('y_test = ', y_test.shape) X_train.head() y_train.head() Ketujuh Menentukan objek kelas knn. knn = KNeighborsClassifier(n_neighbors = 5) Kedelapan Memasukkan nilai data train kedalam fungsi knn. knn.fit(X_train, y_train) Kesembilan Mengecek nilai akurasi dari data test. knn.score(X_test, y_test) Kesepuluh Melakukan ploting data. Mengklasifikasikan data berdasarkan jarak data dengan data tetangga terdekat menggunakan warna plot agar mempermudah membaca data. def plot_fruit_knn(X, y, n_neighbors, weights): X_mat = X[['height', 'width']].as_matrix() y_mat = y.as_matrix() # Create color maps cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF','#AFAFAF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#AFAFAF']) clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) clf.fit(X_mat, y_mat) # Plot the decision boundary by assigning a color in the color map # to each mesh point. mesh_step_size = .01 # step size in the mesh plot_symbol_size = 50 x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1 y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size), np.arange(y_min, y_max, mesh_step_size)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light) # Plot training points plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black') plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) patch0 = mpatches.Patch(color='#FF0000', label='apple') patch1 = mpatches.Patch(color='#00FF00', label='mandarin') patch2 = mpatches.Patch(color='#0000FF', label='orange') patch3 = mpatches.Patch(color='#AFAFAF', label='lemon') plt.legend(handles=[patch0, patch1, patch2, patch3]) plt.xlabel('height (cm)') plt.ylabel('width (cm)') plt.show() plot_fruit_knn(X_train, y_train, 5, 'uniform') # n_neighbors = 5 Kesebelas Melakukan prediksi terhadap beberapa data baru. fruit_prediction = knn.predict([[30, 6, 5]]) lookup_fruit_name[fruit_prediction[0]] Output: mandarin fruit_prediction = knn.predict([[500, 500, 500]]) lookup_fruit_name[fruit_prediction[0]] Output: orange Selanjutnya ulangi langkah kesebelas untuk melakukan test data baru untuk diuji dengan K-Nearest Neighbor Classification. Referensi Gorunescu, F. 2011. Data Mining Concept Model and Techniques. Berlin: Springer. ISBN 978-3-642-19720-8. Han, Jiawei dan Kamber, Micheline. (2006), Data Mining : Concept and Techniques Second Edition, Morgan Kaufmann Publishers. Florin Gorunescu, Data Mining: Concepts, Models and Techniques, Springer, 2011. https://cahyadsn.phpindonesia.id/extra/knn.php https://www.academia.edu/31306621/MAKALAH_KNN_K-NEAREST_NEIGHBOUR_","title":"K-Nearest Neighbors"},{"location":"indeksknn/#k-nearest-neighbors","text":"","title":"K-Nearest Neighbors"},{"location":"indeksknn/#pendahuluan","text":"","title":"Pendahuluan"},{"location":"indeksknn/#pengertian-k-nearest-neighbor","text":"K-Nearest Neighbor (K-NN) adalah suatu metode yang menggunakan algoritma supervised dimana hasil dari sampel uji yang baru diklasifikasikan berdasarkan mayoritas dari kategori pada K-NN. Tujuan dari algoritma ini adalah mengklasifikasi objek baru berdasakan atribut dan sampel latih. pengklasifikasian tidak menggunakan model apapun untuk dicocokkan dan hanya berdasarkan pada memori. Diberikan titik uji, akan ditemukan sejumlah K objek (titik training) yang paling dekat dengan titik uji. Klasifikasi menggunakan voting terbanyak di antara klasifikasi dari K objek. Algoritma K-NN menggunakan klasifikasi ketetanggaan sebagai nilai prediksi dari sample uji yang baru. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan jarak Eucledian. Nilai k yang terbaik untuk algoritma ini tergantung pada data. Secara umum, nilai k yang tinggi akan mengurangi efek noise pada klasifikasi, tetapi membuat batasan antara setiap klasifikasi menjadi lebih kabur. Nilai k yang bagus dapat dipilih dengan optimasi parameter, misalnya dengan menggunakan cross-validation. Kasus khusus di mana klasifikasi diprediksikan berdasarkan data pembelajaran yang paling dekat (dengan kata lain, k = 1) disebut algoritma k-nearest neighbor. Ketepatan algoritma k-NN ini sangat dipengaruhi oleh ada atau tidaknya fitur-fitur yang tidak relevan, atau jika bobot fitur tersebut tidak setara dengan relevansinya terhadap klasifikasi. Riset terhadap algoritma ini sebagian besar membahas bagaimana memilih dan memberi bobot terhadap fitur agar performa klasifikasi menjadi lebih baik. Sesuai dengan prinsip kerja K-Nearest Neighbor yaitu mencari jarak terdekat antara data yang akan dievaluasi dengan k tetangga(neighbor) terdekatnya dalam data pelatihan. Persamaan dibawah ini menunjukkan rumus perhitungan untuk mencari jarak terdekat dengan d adalah jarak dan p adalah dimensi data(Agusta, 2007): Dengan keterangan : \ud835\udc651 : sampel data \ud835\udc652 : data uji i : data ke-i d : jarak euclidean p : dimensi data","title":"Pengertian K-Nearest Neighbor"},{"location":"indeksknn/#algoritma","text":"Algoritma metode KNN sangatlah sederhana, bekerja berdasarkan jarak terpendek dari query instance ke training sample untuk menentukan KNN-nya. Training sample diproyeksikan ke ruang berdimensi banyak, dimana masing-masing dimensi merepresentasikan fitur dari data. Ruang ini dibagi menjadi bagian-bagian berdasarkan klasifikasi training sample. Sebuah titik pada ruang ini ditandai kelac c jika kelas c merupakan klasifikasi yang paling banyak ditemui pada k buah tetangga terdekat dari titik tersebut. Dekat atau jauhnya tetangga biasanya dihitung berdasarkan Euclidean Distance. Tentukan parameter K (jumlah tetangga paling dekat). Hitung kuadrat jarak euclid masing-masing objek terhadap data sample yang diberikan. Urutkan objek-objek kedalam kelompok yang memiliki jarak terkecil. Kumpulkan kategori Y (klasifikasi nearest neighbor). Dengan kategori nearest neighbor yang paling banyak, maka dapt diprediksikan nilai query instance yang telah dihitung.","title":"Algoritma"},{"location":"indeksknn/#contoh-kasus-hitung-manual","text":"Data didapatkan dari kuesioner dengan obyek pengujian berupa dua atribut (daya tahan keasaman dan kekuatan) untuk mengklasifikasikan apakah sebuah kertas tissue tergolong bagus atau jelek. Berikut ini contoh datanya: X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m 2 ) Klasifikasi 7 7 Jelek 7 4 Jelek 3 4 Bagus 1 4 Bagus Sebuah pabrik memproduksi kertas tissue baru yang memiliki X1 = 3 dan X2 = 7. Kita gunakan algoritma KNN untuk melakukan prediksi termasuk klasifikasi apa (bagus atau jelek) kertas tissue yang baru ini. Jawaban Tentukan parameter K = jumlah banyaknya tetangga terdekat. Misal K=3. Hitung jarak antara data baru dan semua data yang ada di data training. Misal digunakan square distance dari jarak antara data baru dengan semua data yang ada di data training. X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) 7 7 (7-3) 2 +(7-7) 2 = 16 7 4 (7-3) 2 +(4-7) 2 = 25 3 4 (3-3) 2 +(4-7) 2 = 9 1 4 (1-3) 2 +(4-7) 2 = 13 Urutkan jarak tersebut dan tentukan tetangga mana yang terdekat berdasarkan jarak minimum ke-K. X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) Urutan Jarak K-3-NN? 7 7 (7-3)2+(7-7)2 = 16 3 Ya 7 4 (7-3)2+(4-7)2 = 25 4 Tidak 3 4 (3-3)2+(4-7)2 = 9 1 Ya 1 4 (1-3)2+(4-7)2 = 13 2 Ya Tentukan kategori dari tetangga terdekat. Perhatikan pada baris kedua pada kolom terakhir: katagori dari tetangga terdekat (Y) tidak termasuk karena ranking dari data ini lebih dari 3 (=K). X1 = Daya tahan keasaman (detik) X2 = Kekuatan (kg/m2) Square Distance ke data baru (3,7) Urutan Jarak K-3-NN? Kategory K-NN 7 7 (7-3)2+(7-7)2 = 16 3 Ya Jelek 7 4 (7-3)2+(4-7)2 = 25 4 Tidak - 3 4 (3-3)2+(4-7)2 = 9 1 Ya Bagus 1 4 (1-3)2+(4-7)2 = 13 2 Ya Bagus Gunakan kategori mayoritas yang sederhana dari tetangga yang terdekat tersebut sebagai nilai prediksi dari data yang baru. Kita punya 2 kategori Bagus dan 1 kategori Jelek, karena 2>1 maka kita simpulkan bahwa kertas tissue baru tadi yang memiliki X1 = 3 dan X2 = 7 termasuk dalam kategori Bagus. Itulah contoh penerapan k-nn pada sebuah kasus kusioner terhadap kertas tissue.","title":"Contoh Kasus (Hitung Manual)"},{"location":"indeksknn/#kelebihan","text":"Mudah dipahami dan diimplementasikan. Sangan non-linear . K-NN merupakan salah satu algoritma (model) pembelajaran mesin yang bersifat non-parametrik , yaitu model yang tidak mengasumsikan apa-apa mengenai distribusi instance di dalam dataset . Model non-parametrik biasanya lebih sulit diinterpretasikan, namun salah satu kelebihannya adalah garis keputusan kelas yang dihasilkan model tersebut bisa jadi sangat fleksibel dan non-linear . Kuat dalam hal ruang pencarian, misalnya kelas tidak harus linear dipisahkan. Efektif apabila data training sample -nya besar. Tangguh terhadap data training sample -nya besar. Beberapa parameter untuk acuan: jarak metrik dan k. Memiliki konsistensi yang kuat.","title":"Kelebihan"},{"location":"indeksknn/#kekurangan","text":"Perlu untuk menentukan nilai k yang optimal sehingga untuk menyatakan jumlah tetangga terdekatnya lebih mudah. Nilai komputasi yang cukup tinggi karena perhitungan jarak harus dilkukan pada setiap query instance. Tidak menangani missing value secara implisit. Sensitif terhadap data pencilan ( outlier ). Rentan terhadap variabel yang non-informatif. Gambar : Variabel yang noninformatif mengacaukan klasifikasi dalam k-nn Rentan terhadap dimensionalitas yang tinggi. Rentang terhadap perbedaan rentang variabel. Gambar : Perbedaan rentang variabel bisa mengacaukan klasifikasi k-nn","title":"Kekurangan"},{"location":"indeksknn/#implementasi-studi-kasus","text":"","title":"Implementasi (Studi Kasus)"},{"location":"indeksknn/#alat-bahan","text":"Sebelum menerapkan konsep k-nearest neighbor pada studi kasus yang telah ditentukan, beberapa tools yang perlu dipersiapkan agar program yang kita rancang bisa dieksekusi dengan baik diantaranya: python 3.x (versi 3 keatas). Anaconda Navigator atau Pycharm. Untuk mempermudah kawan-kawan mendapatkan toolsnya, sillakan kawan-kawan bisa download tools-nya disini . Studi kasus pada Fruit Data adalah mengklasifikasikan beberapa jenis buah dengan 3 ketentuan inputan. Untuk contoh penulisan program dan datanya bisa mengambil kawan-kawan download disini . Berikut source code dan penjelasan untuk menyelesaikan study kasus tersebut dengan K-NN Classification.","title":"Alat &amp; Bahan"},{"location":"indeksknn/#langkah-langkah","text":"","title":"Langkah-langkah"},{"location":"indeksknn/#pertama","text":"Import beberapa library dari python seperti: pandas => memuat sebuah file ke dalam tabel virtual ala spreadsheet yang memiliki struktur data yang diperukan untuk membersihkan data mentah ke dalam sebuah bentuk yang cocok untuk dianalisis. numpy => untuk operasi vektor dan matriks. Fiturnya hampir sama dengan MATLAB dalam mengelola array dan array multidimensi. sklearn => untuk mengimportkan library data science . Berbagai fungsi didalamnya seperti fungsi agregasi, hitung metriks, hitung akurasi, display gambar, dan lain sebagainya. matplotlib => untuk menyajikan visualisasi data cluster . #import library import numpy as np import matplotlib.pyplot as plt import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn import neighbors from matplotlib.colors import ListedColormap, BoundaryNorm import matplotlib.patches as mpatches","title":"Pertama"},{"location":"indeksknn/#kedua","text":"Mengimport data dari komputer dengan perintah pandas. #read data data = pd.read_table('fruit_data.txt') #Pastikan file data set berada dalam folder yang sama dengan file jupyter notebook","title":"Kedua"},{"location":"indeksknn/#ketiga","text":"Menampilkan data. #explore data print(data.shape) data.head(10) #Menampilkan 10 baris pertama dari tabel # membuat nilai kunci utama antara fruit_label dengan fruit_name lookup_fruit_name = dict(zip(data.fruit_label.unique(), data.fruit_name.unique())) print(lookup_fruit_name)","title":"Ketiga"},{"location":"indeksknn/#keempat","text":"Membuat model dan data train. X = data[['mass', 'width', 'height']] y = data['fruit_label']","title":"Keempat"},{"location":"indeksknn/#kelima","text":"Melakukan split (memisah) data antara data test dan data train. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=0)","title":"Kelima"},{"location":"indeksknn/#keenam","text":"Mengecek nilai dengan dimensi array. print('X_train = ', X_train.shape) print('X_test = ', X_test.shape) print('y_train = ', y_train.shape) print('y_test = ', y_test.shape) X_train.head() y_train.head()","title":"Keenam"},{"location":"indeksknn/#ketujuh","text":"Menentukan objek kelas knn. knn = KNeighborsClassifier(n_neighbors = 5)","title":"Ketujuh"},{"location":"indeksknn/#kedelapan","text":"Memasukkan nilai data train kedalam fungsi knn. knn.fit(X_train, y_train)","title":"Kedelapan"},{"location":"indeksknn/#kesembilan","text":"Mengecek nilai akurasi dari data test. knn.score(X_test, y_test)","title":"Kesembilan"},{"location":"indeksknn/#kesepuluh","text":"Melakukan ploting data. Mengklasifikasikan data berdasarkan jarak data dengan data tetangga terdekat menggunakan warna plot agar mempermudah membaca data. def plot_fruit_knn(X, y, n_neighbors, weights): X_mat = X[['height', 'width']].as_matrix() y_mat = y.as_matrix() # Create color maps cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF','#AFAFAF']) cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#AFAFAF']) clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) clf.fit(X_mat, y_mat) # Plot the decision boundary by assigning a color in the color map # to each mesh point. mesh_step_size = .01 # step size in the mesh plot_symbol_size = 50 x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1 y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size), np.arange(y_min, y_max, mesh_step_size)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light) # Plot training points plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, cmap=cmap_bold, edgecolor = 'black') plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) patch0 = mpatches.Patch(color='#FF0000', label='apple') patch1 = mpatches.Patch(color='#00FF00', label='mandarin') patch2 = mpatches.Patch(color='#0000FF', label='orange') patch3 = mpatches.Patch(color='#AFAFAF', label='lemon') plt.legend(handles=[patch0, patch1, patch2, patch3]) plt.xlabel('height (cm)') plt.ylabel('width (cm)') plt.show() plot_fruit_knn(X_train, y_train, 5, 'uniform') # n_neighbors = 5","title":"Kesepuluh"},{"location":"indeksknn/#kesebelas","text":"Melakukan prediksi terhadap beberapa data baru. fruit_prediction = knn.predict([[30, 6, 5]]) lookup_fruit_name[fruit_prediction[0]] Output: mandarin fruit_prediction = knn.predict([[500, 500, 500]]) lookup_fruit_name[fruit_prediction[0]] Output: orange Selanjutnya ulangi langkah kesebelas untuk melakukan test data baru untuk diuji dengan K-Nearest Neighbor Classification.","title":"Kesebelas"},{"location":"indeksknn/#referensi","text":"Gorunescu, F. 2011. Data Mining Concept Model and Techniques. Berlin: Springer. ISBN 978-3-642-19720-8. Han, Jiawei dan Kamber, Micheline. (2006), Data Mining : Concept and Techniques Second Edition, Morgan Kaufmann Publishers. Florin Gorunescu, Data Mining: Concepts, Models and Techniques, Springer, 2011. https://cahyadsn.phpindonesia.id/extra/knn.php https://www.academia.edu/31306621/MAKALAH_KNN_K-NEAREST_NEIGHBOUR_","title":"Referensi"}]}